{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "from geopy.geocoders import ArcGIS, Bing, Nominatim, OpenCage, GoogleV3, OpenMapQuest\n",
    "import csv, sys\n",
    "from geopy.geocoders import Nominatim \n",
    "import multiprocessing\n",
    "import random as rd\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from time import sleep\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "import logging\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, wordpunct_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=os.path.join(\"..\",\"data\",\"nlp-disaster\",\"train.csv\")\n",
    "data=pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>new_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target new_location  \n",
       "0       1       Italia  \n",
       "1       1       Italia  \n",
       "2       1       Italia  \n",
       "3       1       Italia  \n",
       "4       1       Italia  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"target\"]==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>new_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                          text  target new_location\n",
       "15  23     NaN      NaN                What's up man?       0       Italia\n",
       "16  24     NaN      NaN                 I love fruits       0       Italia\n",
       "17  25     NaN      NaN              Summer is lovely       0       Italia\n",
       "18  26     NaN      NaN             My car is so fast       0       Italia\n",
       "19  28     NaN      NaN  What a goooooooaaaaaal!!!!!!       0       Italia"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"target\"]==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(data, title='Profiling Report', explorative=True)\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [1:14:21<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "arcgis = ArcGIS(timeout=50)\n",
    "nominatim = Nominatim(timeout=50,user_agent=\"bogchalaca\")\n",
    "googlev3 = GoogleV3(timeout=50)\n",
    "\n",
    "# choose and order your preference for geocoders here\n",
    "geocoders = [nominatim]\n",
    "def geocode(address):\n",
    "    i = 0\n",
    "    try:\n",
    "        while i < len(geocoders):\n",
    "            # try to geocode using a service\n",
    "            location = geocoders[i].geocode(address)\n",
    "            # if it returns a location\n",
    "            if location != None:\n",
    "                \n",
    "                country=location[0].split(\",\")[-1]\n",
    "                return country\n",
    "            else:\n",
    "                i += 1\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "#addresses = pool.map(geocode_worker, data[\"location\"])\n",
    "result=[]\n",
    "for _ in tqdm(pool.imap(geocode, data[\"location\"],chunksize=1), total=len(data[\"location\"])):\n",
    "    result.append(_)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('location.pkl', 'wb') as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"location.pkl\",\"rb\") as f:\n",
    "    result=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result=pd.DataFrame(result)\n",
    "new_result.replace(\"Italia\",\"None\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"new_location\"]=new_result\n",
    "data_sample=data[data[\"keyword\"].notna()].reset_index(drop=True)\n",
    "final_data=pd.concat([data_sample,pd.get_dummies(data_sample[\"new_location\"])],axis=1)\n",
    "final_data=pd.concat([data_sample,pd.get_dummies(data_sample[\"keyword\"],prefix=\"keyword\")],axis=1)\n",
    "y=final_data[\"target\"]\n",
    "text=final_data[\"text\"]\n",
    "final_data.drop([\"keyword\",\"new_location\",\"id\",\"location\",\"target\",\"text\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def clean_data(quote):\n",
    "    quote = quote.lower()\n",
    "    tokens = word_tokenize(quote)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_words = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    token_punc = [t for t in lem_words if t.isalpha()]\n",
    "    token_stop = [t for t in token_punc if t not in stop_words]\n",
    "    return \" \".join(token_stop)\n",
    "\n",
    "\n",
    "text = pd.DataFrame(text.apply(lambda x: clean_data(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',min_df=0.01)\n",
    "tf_idf = vectorizer.fit_transform(text[\"text\"]).toarray()\n",
    "tf_idf=pd.DataFrame(tf_idf,columns=vectorizer.get_feature_names())\n",
    "final_data=pd.concat([final_data,tf_idf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword_ablaze</th>\n",
       "      <th>keyword_accident</th>\n",
       "      <th>keyword_aftershock</th>\n",
       "      <th>keyword_airplane%20accident</th>\n",
       "      <th>keyword_ambulance</th>\n",
       "      <th>keyword_annihilated</th>\n",
       "      <th>keyword_annihilation</th>\n",
       "      <th>keyword_apocalypse</th>\n",
       "      <th>keyword_armageddon</th>\n",
       "      <th>keyword_army</th>\n",
       "      <th>...</th>\n",
       "      <th>war</th>\n",
       "      <th>watch</th>\n",
       "      <th>way</th>\n",
       "      <th>weapon</th>\n",
       "      <th>wildfire</th>\n",
       "      <th>woman</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword_ablaze  keyword_accident  keyword_aftershock  \\\n",
       "0               1                 0                   0   \n",
       "1               1                 0                   0   \n",
       "2               1                 0                   0   \n",
       "3               1                 0                   0   \n",
       "4               1                 0                   0   \n",
       "\n",
       "   keyword_airplane%20accident  keyword_ambulance  keyword_annihilated  \\\n",
       "0                            0                  0                    0   \n",
       "1                            0                  0                    0   \n",
       "2                            0                  0                    0   \n",
       "3                            0                  0                    0   \n",
       "4                            0                  0                    0   \n",
       "\n",
       "   keyword_annihilation  keyword_apocalypse  keyword_armageddon  keyword_army  \\\n",
       "0                     0                   0                   0             0   \n",
       "1                     0                   0                   0             0   \n",
       "2                     0                   0                   0             0   \n",
       "3                     0                   0                   0             0   \n",
       "4                     0                   0                   0             0   \n",
       "\n",
       "   ...  war  watch  way  weapon  wildfire  woman  work  world  year  youtube  \n",
       "0  ...  0.0    0.0  0.0     0.0       0.0    0.0   0.0    0.0   0.0      0.0  \n",
       "1  ...  0.0    0.0  0.0     0.0       0.0    0.0   0.0    0.0   0.0      0.0  \n",
       "2  ...  0.0    0.0  0.0     0.0       0.0    0.0   0.0    0.0   0.0      0.0  \n",
       "3  ...  0.0    0.0  0.0     0.0       0.0    0.0   0.0    0.0   0.0      0.0  \n",
       "4  ...  0.0    0.0  0.0     0.0       0.0    0.0   0.0    0.0   0.0      0.0  \n",
       "\n",
       "[5 rows x 285 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7552, 285)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "final_data[\"polarity\"] = data_sample[\"text\"].apply(pol)\n",
    "final_data[\"subjectivity\"] = data_sample[\"text\"].apply(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('final_data.pkl', 'wb') as f:\n",
    "    pickle.dump(final_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"final_data.pkl\",\"rb\") as f:\n",
    "    final_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7399073461283918"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train,X_test,y_train,y_test=tts(final_data,y,train_size=0.8)\n",
    "rfc=RandomForestClassifier(n_estimators=10,max_depth=30)\n",
    "rfc.fit(X_train,y_train)\n",
    "ypred=rfc.predict(X_test)\n",
    "accuracy_score(y_test,ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7557908669755129"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "lgb=LGBMClassifier(learning_rate=0.5)\n",
    "lgb.fit(X_train,y_train)\n",
    "ypred=lgb.predict(X_test)\n",
    "accuracy_score(y_test,ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a function\n",
    "def model(input_dim):\n",
    "    # Create the Sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "  \n",
    "    \n",
    "    # Use a relu activation function\n",
    "    model.add(tf.keras.layers.Dense(50, input_dim=input_dim, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "    # Final layer is sigmoid for binary classification\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # return the model\n",
    "    return model  \n",
    "\n",
    "my_model=model(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "378/378 [==============================] - 2s 4ms/step - loss: 0.6252 - accuracy: 0.6319 - val_loss: 0.5108 - val_accuracy: 0.7545\n",
      "Epoch 2/20\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.4560 - accuracy: 0.7934 - val_loss: 0.5018 - val_accuracy: 0.7651\n",
      "Epoch 3/20\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.4261 - accuracy: 0.8066 - val_loss: 0.5005 - val_accuracy: 0.7677\n",
      "Epoch 4/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.4006 - accuracy: 0.8173 - val_loss: 0.5286 - val_accuracy: 0.7604\n",
      "Epoch 5/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.3766 - accuracy: 0.8337 - val_loss: 0.5282 - val_accuracy: 0.7578\n",
      "Epoch 6/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.3547 - accuracy: 0.8390 - val_loss: 0.5654 - val_accuracy: 0.7538\n",
      "Epoch 7/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.3252 - accuracy: 0.8570 - val_loss: 0.5778 - val_accuracy: 0.7505\n",
      "Epoch 8/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.3059 - accuracy: 0.8662 - val_loss: 0.6045 - val_accuracy: 0.7498\n",
      "Epoch 9/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2815 - accuracy: 0.8688 - val_loss: 0.6500 - val_accuracy: 0.7478\n",
      "Epoch 10/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2704 - accuracy: 0.8767 - val_loss: 0.7505 - val_accuracy: 0.7531\n",
      "Epoch 11/20\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2472 - accuracy: 0.8846 - val_loss: 0.7247 - val_accuracy: 0.7518\n",
      "Epoch 12/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2267 - accuracy: 0.8981 - val_loss: 0.7474 - val_accuracy: 0.7551\n",
      "Epoch 13/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2306 - accuracy: 0.8915 - val_loss: 0.8441 - val_accuracy: 0.7631\n",
      "Epoch 14/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2157 - accuracy: 0.8960 - val_loss: 0.8597 - val_accuracy: 0.7426\n",
      "Epoch 15/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2030 - accuracy: 0.9057 - val_loss: 1.0184 - val_accuracy: 0.7565\n",
      "Epoch 16/20\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.1967 - accuracy: 0.9042 - val_loss: 0.9684 - val_accuracy: 0.7426\n",
      "Epoch 17/20\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2016 - accuracy: 0.9028 - val_loss: 0.9359 - val_accuracy: 0.7485\n",
      "Epoch 18/20\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9106 - val_loss: 1.0644 - val_accuracy: 0.7459\n",
      "Epoch 19/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.1878 - accuracy: 0.9077 - val_loss: 1.1709 - val_accuracy: 0.7439\n",
      "Epoch 20/20\n",
      "378/378 [==============================] - 2s 5ms/step - loss: 0.1823 - accuracy: 0.9151 - val_loss: 1.0676 - val_accuracy: 0.7439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7438782263401721"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "my_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "            TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)]\n",
    "my_model.fit(X_train,y_train,validation_data=(X_test, y_test), epochs=20, batch_size=16,callbacks=callbacks)\n",
    "ypred = (my_model.predict(X_test)>=0.5)*1\n",
    "accuracy_score(y_test,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7552 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 7552/7552 [00:06<00:00, 1094.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def process_comments(tokenizer, comments, max_length):\n",
    "    input_ids, attention_mask = [], []\n",
    "    for comment in tqdm(comments):\n",
    "        proccessed_comment = tokenizer.encode_plus(comment, max_length=max_length, pad_to_max_length=True)\n",
    "        input_ids.append(proccessed_comment[\"input_ids\"])\n",
    "        attention_mask.append(proccessed_comment[\"attention_mask\"])\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "max_length=50\n",
    "input_ids, attention_mask = process_comments(tokenizer, data_sample[\"text\"], max_length)\n",
    "input_ids=torch.tensor(input_ids)\n",
    "attention_mask=torch.tensor(attention_mask)\n",
    "y = torch.tensor(y,dtype=torch.float32)\n",
    "y=torch.reshape(y,(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "\n",
    "(train_inputs,test_inputs, \n",
    " train_mask, test_mask, \n",
    " train_targets, test_targets) = train_test_split(input_ids, attention_mask, y, test_size=0.5)\n",
    "train_data = TensorDataset(train_inputs, train_mask, train_targets)\n",
    "test_data = TensorDataset(test_inputs, test_mask, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7552])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/.pyenv/versions/3.7.6/envs/vivadata/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d15468253d4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=1)\n",
    "train_loader = DataLoader(train_data,batch_size=8, shuffle=True)\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer =torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    count=0\n",
    "    for X_batch,X_attention_batch,y_batch in train_loader:\n",
    "        print(count)\n",
    "        output =   model(X_batch,attention_mask=X_attention_batch,labels=None)\n",
    "        y_pred = output[0]\n",
    "        y_pred = torch.reshape(y_pred,(-1,))\n",
    "        loss = loss_fn(y_pred,y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        count+=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test, X_test_attention)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))preds = np.zeros([len(test_dataset), 1])\n",
    "model.eval()\n",
    "for i, (x_batch, x_mask) in enumerate(test_loader):\n",
    "    outputs = model(x_batch.to(device),attention_mask=x_mask.to(device))\n",
    "    y_pred = sigmoid(outputs[0].detach().cpu().numpy())\n",
    "    preds[i*16:(i+1)*16, :] = y_predprint(metrics.roc_auc_score(y_test, preds))\n",
    "\n",
    "print(metrics.roc_auc_score(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
