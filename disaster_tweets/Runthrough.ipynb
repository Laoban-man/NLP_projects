{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "from geopy.geocoders import ArcGIS, Bing, Nominatim, OpenCage, GoogleV3, OpenMapQuest\n",
    "import csv, sys\n",
    "from geopy.geocoders import Nominatim \n",
    "import multiprocessing\n",
    "import random as rd\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from time import sleep\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "import logging\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, wordpunct_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=os.path.join(\"..\",\"..\",\"data\",\"nlp-disaster\",\"train.csv\")\n",
    "data=pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>new_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target new_location  \n",
       "0       1       Italia  \n",
       "1       1       Italia  \n",
       "2       1       Italia  \n",
       "3       1       Italia  \n",
       "4       1       Italia  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"target\"]==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>new_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>Italia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                          text  target new_location\n",
       "15  23     NaN      NaN                What's up man?       0       Italia\n",
       "16  24     NaN      NaN                 I love fruits       0       Italia\n",
       "17  25     NaN      NaN              Summer is lovely       0       Italia\n",
       "18  26     NaN      NaN             My car is so fast       0       Italia\n",
       "19  28     NaN      NaN  What a goooooooaaaaaal!!!!!!       0       Italia"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"target\"]==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f4e174eb7d4a7d8562956fc0e0b0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Summarize dataset'), FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fb487e67404577b95ff2931287ace6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generate report structure'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da554f55ba814ebab2005d6334e84394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Render HTML'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c06edda836d499c88b83d70d4a22dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Export report to file'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "profile = ProfileReport(data, title='Profiling Report', explorative=True)\n",
    "profile.to_file(\"static/eda.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [1:14:21<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "arcgis = ArcGIS(timeout=50)\n",
    "nominatim = Nominatim(timeout=50,user_agent=\"bogchalaca\")\n",
    "googlev3 = GoogleV3(timeout=50)\n",
    "\n",
    "# choose and order your preference for geocoders here\n",
    "geocoders = [nominatim]\n",
    "def geocode(address):\n",
    "    i = 0\n",
    "    try:\n",
    "        while i < len(geocoders):\n",
    "            # try to geocode using a service\n",
    "            location = geocoders[i].geocode(address)\n",
    "            # if it returns a location\n",
    "            if location != None:\n",
    "                \n",
    "                country=location[0].split(\",\")[-1]\n",
    "                return country\n",
    "            else:\n",
    "                i += 1\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "#addresses = pool.map(geocode_worker, data[\"location\"])\n",
    "result=[]\n",
    "for _ in tqdm(pool.imap(geocode, data[\"location\"],chunksize=1), total=len(data[\"location\"])):\n",
    "    result.append(_)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('location.pkl', 'wb') as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/location.pkl\",\"rb\") as f:\n",
    "    result=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result=pd.DataFrame(result)\n",
    "new_result.replace(\"Italia\",\"None\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"new_location\"]=new_result\n",
    "data_sample=data[data[\"keyword\"].notna()].reset_index(drop=True)\n",
    "final_data=pd.concat([data_sample,pd.get_dummies(data_sample[\"new_location\"])],axis=1)\n",
    "final_data=pd.concat([data_sample,pd.get_dummies(data_sample[\"keyword\"],prefix=\"keyword\")],axis=1)\n",
    "y=final_data[\"target\"]\n",
    "text=final_data[\"text\"]\n",
    "final_data.drop([\"keyword\",\"new_location\",\"id\",\"location\",\"target\",\"text\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7552,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def clean_data(quote):\n",
    "    quote = quote.lower()\n",
    "    tokens = word_tokenize(quote)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_words = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    token_punc = [t for t in lem_words if t.isalpha()]\n",
    "    token_stop = [t for t in token_punc if t not in stop_words]\n",
    "    return \" \".join(token_stop)\n",
    "\n",
    "\n",
    "text = pd.DataFrame(text.apply(lambda x: clean_data(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',min_df=0.01)\n",
    "tf_idf = vectorizer.fit_transform(text[\"text\"]).toarray()\n",
    "tf_idf=pd.DataFrame(tf_idf,columns=vectorizer.get_feature_names())\n",
    "final_data=pd.concat([final_data,tf_idf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword_ablaze</th>\n",
       "      <th>keyword_accident</th>\n",
       "      <th>keyword_aftershock</th>\n",
       "      <th>keyword_airplane%20accident</th>\n",
       "      <th>keyword_ambulance</th>\n",
       "      <th>keyword_annihilated</th>\n",
       "      <th>keyword_annihilation</th>\n",
       "      <th>keyword_apocalypse</th>\n",
       "      <th>keyword_armageddon</th>\n",
       "      <th>keyword_army</th>\n",
       "      <th>...</th>\n",
       "      <th>keyword_weapons</th>\n",
       "      <th>keyword_whirlwind</th>\n",
       "      <th>keyword_wild%20fires</th>\n",
       "      <th>keyword_wildfire</th>\n",
       "      <th>keyword_windstorm</th>\n",
       "      <th>keyword_wounded</th>\n",
       "      <th>keyword_wounds</th>\n",
       "      <th>keyword_wreck</th>\n",
       "      <th>keyword_wreckage</th>\n",
       "      <th>keyword_wrecked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword_ablaze  keyword_accident  keyword_aftershock  \\\n",
       "0               1                 0                   0   \n",
       "1               1                 0                   0   \n",
       "2               1                 0                   0   \n",
       "3               1                 0                   0   \n",
       "4               1                 0                   0   \n",
       "\n",
       "   keyword_airplane%20accident  keyword_ambulance  keyword_annihilated  \\\n",
       "0                            0                  0                    0   \n",
       "1                            0                  0                    0   \n",
       "2                            0                  0                    0   \n",
       "3                            0                  0                    0   \n",
       "4                            0                  0                    0   \n",
       "\n",
       "   keyword_annihilation  keyword_apocalypse  keyword_armageddon  keyword_army  \\\n",
       "0                     0                   0                   0             0   \n",
       "1                     0                   0                   0             0   \n",
       "2                     0                   0                   0             0   \n",
       "3                     0                   0                   0             0   \n",
       "4                     0                   0                   0             0   \n",
       "\n",
       "   ...  keyword_weapons  keyword_whirlwind  keyword_wild%20fires  \\\n",
       "0  ...                0                  0                     0   \n",
       "1  ...                0                  0                     0   \n",
       "2  ...                0                  0                     0   \n",
       "3  ...                0                  0                     0   \n",
       "4  ...                0                  0                     0   \n",
       "\n",
       "   keyword_wildfire  keyword_windstorm  keyword_wounded  keyword_wounds  \\\n",
       "0                 0                  0                0               0   \n",
       "1                 0                  0                0               0   \n",
       "2                 0                  0                0               0   \n",
       "3                 0                  0                0               0   \n",
       "4                 0                  0                0               0   \n",
       "\n",
       "   keyword_wreck  keyword_wreckage  keyword_wrecked  \n",
       "0              0                 0                0  \n",
       "1              0                 0                0  \n",
       "2              0                 0                0  \n",
       "3              0                 0                0  \n",
       "4              0                 0                0  \n",
       "\n",
       "[5 rows x 221 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cleaned_text.pkl', 'wb') as f:\n",
    "    pickle.dump(text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "final_data[\"polarity\"] = data_sample[\"text\"].apply(pol)\n",
    "final_data[\"subjectivity\"] = data_sample[\"text\"].apply(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7552, 221)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('final_data.pkl', 'wb') as f:\n",
    "    pickle.dump(final_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/final_data.pkl\",\"rb\") as f:\n",
    "    final_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=final_data.iloc[10,:-2]\n",
    "rfc.predict(np.array(temp).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7246856386499008"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train,X_test,y_train,y_test=tts(final_data,y,train_size=0.8)\n",
    "rfc=RandomForestClassifier(n_estimators=10,max_depth=30)\n",
    "rfc.fit(X_train,y_train)\n",
    "ypred=rfc.predict(X_test)\n",
    "accuracy_score(y_test,ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc= pickle.load(open(\"models/rfc.sav\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 30,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7412309728656519"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "lgb=LGBMClassifier(learning_rate=0.5)\n",
    "lgb.fit(X_train,y_train)\n",
    "ypred=lgb.predict(X_test)\n",
    "accuracy_score(y_test,ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rfc, open(\"rfc.sav\", 'wb'))\n",
    "pickle.dump(lgb, open(\"lgb.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a function\n",
    "def model(input_dim):\n",
    "    # Create the Sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "  \n",
    "    \n",
    "    # Use a relu activation function\n",
    "    model.add(tf.keras.layers.Dense(50, input_dim=input_dim, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "    # Final layer is sigmoid for binary classification\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # return the model\n",
    "    return model  \n",
    "\n",
    "my_model=model(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "378/378 [==============================] - 2s 3ms/step - loss: 0.6251 - accuracy: 0.6313 - val_loss: 0.5222 - val_accuracy: 0.7478\n",
      "Epoch 2/20\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.4574 - accuracy: 0.7935 - val_loss: 0.5153 - val_accuracy: 0.7591\n",
      "Epoch 3/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.4117 - accuracy: 0.8178 - val_loss: 0.5278 - val_accuracy: 0.7498\n",
      "Epoch 4/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.4098 - accuracy: 0.8159 - val_loss: 0.5517 - val_accuracy: 0.7472\n",
      "Epoch 5/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.3678 - accuracy: 0.8354 - val_loss: 0.5471 - val_accuracy: 0.7518\n",
      "Epoch 6/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.3575 - accuracy: 0.8414 - val_loss: 0.6043 - val_accuracy: 0.7432\n",
      "Epoch 7/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.3333 - accuracy: 0.8512 - val_loss: 0.6270 - val_accuracy: 0.7459\n",
      "Epoch 8/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.3135 - accuracy: 0.8555 - val_loss: 0.6515 - val_accuracy: 0.7399\n",
      "Epoch 9/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.3106 - accuracy: 0.8591 - val_loss: 0.6861 - val_accuracy: 0.7359\n",
      "Epoch 10/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2824 - accuracy: 0.8699 - val_loss: 0.7191 - val_accuracy: 0.7313\n",
      "Epoch 11/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2847 - accuracy: 0.8654 - val_loss: 0.7397 - val_accuracy: 0.7320\n",
      "Epoch 12/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2743 - accuracy: 0.8691 - val_loss: 0.8129 - val_accuracy: 0.7366\n",
      "Epoch 13/20\n",
      "378/378 [==============================] - 1s 4ms/step - loss: 0.2677 - accuracy: 0.8794 - val_loss: 0.8687 - val_accuracy: 0.7432\n",
      "Epoch 14/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2693 - accuracy: 0.8718 - val_loss: 0.8774 - val_accuracy: 0.7320\n",
      "Epoch 15/20\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 0.2610 - accuracy: 0.8764 - val_loss: 0.8881 - val_accuracy: 0.7392\n",
      "Epoch 16/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2586 - accuracy: 0.8836 - val_loss: 1.0013 - val_accuracy: 0.7406\n",
      "Epoch 17/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2609 - accuracy: 0.8747 - val_loss: 1.0370 - val_accuracy: 0.7392\n",
      "Epoch 18/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2454 - accuracy: 0.8814 - val_loss: 1.0517 - val_accuracy: 0.7445\n",
      "Epoch 19/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2454 - accuracy: 0.8885 - val_loss: 1.0531 - val_accuracy: 0.7426\n",
      "Epoch 20/20\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 0.2419 - accuracy: 0.8812 - val_loss: 1.2141 - val_accuracy: 0.7412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7412309728656519"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "my_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "            TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)]\n",
    "my_model.fit(X_train,y_train,validation_data=(X_test, y_test), epochs=20, batch_size=16,callbacks=callbacks)\n",
    "ypred = (my_model.predict(X_test)>=0.5)*1\n",
    "accuracy_score(y_test,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/tnn/assets\n"
     ]
    }
   ],
   "source": [
    "my_model.save('models/tnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7552 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 7552/7552 [00:07<00:00, 985.14it/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def process_comments(tokenizer, comments, max_length):\n",
    "    input_ids, attention_mask = [], []\n",
    "    for comment in tqdm(comments):\n",
    "        proccessed_comment = tokenizer.encode_plus(comment, max_length=max_length, pad_to_max_length=True)\n",
    "        input_ids.append(proccessed_comment[\"input_ids\"])\n",
    "        attention_mask.append(proccessed_comment[\"attention_mask\"])\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "max_length=50\n",
    "input_ids, attention_mask = process_comments(tokenizer, data_sample[\"text\"], max_length)\n",
    "input_ids=torch.tensor(input_ids)\n",
    "attention_mask=torch.tensor(attention_mask)\n",
    "y = torch.tensor(y,dtype=torch.float32)\n",
    "y=torch.reshape(y,(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "\n",
    "(train_inputs,test_inputs, \n",
    " train_mask, test_mask, \n",
    " train_targets, test_targets) = train_test_split(input_ids, attention_mask, y, test_size=0.5)\n",
    "train_data = TensorDataset(train_inputs, train_mask, train_targets)\n",
    "test_data = TensorDataset(test_inputs, test_mask, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8\n",
      "16\n",
      "24\n",
      "32\n",
      "40\n",
      "48\n",
      "56\n",
      "64\n",
      "72\n",
      "80\n",
      "88\n",
      "96\n",
      "104\n",
      "112\n",
      "120\n",
      "128\n",
      "136\n",
      "144\n",
      "152\n",
      "160\n",
      "168\n",
      "176\n",
      "184\n",
      "192\n",
      "200\n",
      "208\n",
      "216\n",
      "224\n",
      "232\n",
      "240\n",
      "248\n",
      "256\n",
      "264\n",
      "272\n",
      "280\n",
      "288\n",
      "296\n",
      "304\n",
      "312\n",
      "320\n",
      "328\n",
      "336\n",
      "344\n",
      "352\n",
      "360\n",
      "368\n",
      "376\n",
      "384\n",
      "392\n",
      "400\n",
      "408\n",
      "416\n",
      "424\n",
      "432\n",
      "440\n",
      "448\n",
      "456\n",
      "464\n",
      "472\n",
      "480\n",
      "488\n",
      "496\n",
      "504\n",
      "512\n",
      "520\n",
      "528\n",
      "536\n",
      "544\n",
      "552\n",
      "560\n",
      "568\n",
      "576\n",
      "584\n",
      "592\n",
      "600\n",
      "608\n",
      "616\n",
      "624\n",
      "632\n",
      "640\n",
      "648\n",
      "656\n",
      "664\n",
      "672\n",
      "680\n",
      "688\n",
      "696\n",
      "704\n",
      "712\n",
      "720\n",
      "728\n",
      "736\n",
      "744\n",
      "752\n",
      "760\n",
      "768\n",
      "776\n",
      "784\n",
      "792\n",
      "800\n",
      "808\n",
      "816\n",
      "824\n",
      "832\n",
      "840\n",
      "848\n",
      "856\n",
      "864\n",
      "872\n",
      "880\n",
      "888\n",
      "896\n",
      "904\n",
      "912\n",
      "920\n",
      "928\n",
      "936\n",
      "944\n",
      "952\n",
      "960\n",
      "968\n",
      "976\n",
      "984\n",
      "992\n",
      "1000\n",
      "1008\n",
      "1016\n",
      "1024\n",
      "1032\n",
      "1040\n",
      "1048\n",
      "1056\n",
      "1064\n",
      "1072\n",
      "1080\n",
      "1088\n",
      "1096\n",
      "1104\n",
      "1112\n",
      "1120\n",
      "1128\n",
      "1136\n",
      "1144\n",
      "1152\n",
      "1160\n",
      "1168\n",
      "1176\n",
      "1184\n",
      "1192\n",
      "1200\n",
      "1208\n",
      "1216\n",
      "1224\n",
      "1232\n",
      "1240\n",
      "1248\n",
      "1256\n",
      "1264\n",
      "1272\n",
      "1280\n",
      "1288\n",
      "1296\n",
      "1304\n",
      "1312\n",
      "1320\n",
      "1328\n",
      "1336\n",
      "1344\n",
      "1352\n",
      "1360\n",
      "1368\n",
      "1376\n",
      "1384\n",
      "1392\n",
      "1400\n",
      "1408\n",
      "1416\n",
      "1424\n",
      "1432\n",
      "1440\n",
      "1448\n",
      "1456\n",
      "1464\n",
      "1472\n",
      "1480\n",
      "1488\n",
      "1496\n",
      "1504\n",
      "1512\n",
      "1520\n",
      "1528\n",
      "1536\n",
      "1544\n",
      "1552\n",
      "1560\n",
      "1568\n",
      "1576\n",
      "1584\n",
      "1592\n",
      "1600\n",
      "1608\n",
      "1616\n",
      "1624\n",
      "1632\n",
      "1640\n",
      "1648\n",
      "1656\n",
      "1664\n",
      "1672\n",
      "1680\n",
      "1688\n",
      "1696\n",
      "1704\n",
      "1712\n",
      "1720\n",
      "1728\n",
      "1736\n",
      "1744\n",
      "1752\n",
      "1760\n",
      "1768\n",
      "1776\n",
      "1784\n",
      "1792\n",
      "1800\n",
      "1808\n",
      "1816\n",
      "1824\n",
      "1832\n",
      "1840\n",
      "1848\n",
      "1856\n",
      "1864\n",
      "1872\n",
      "1880\n",
      "1888\n",
      "1896\n",
      "1904\n",
      "1912\n",
      "1920\n",
      "1928\n",
      "1936\n",
      "1944\n",
      "1952\n",
      "1960\n",
      "1968\n",
      "1976\n",
      "1984\n",
      "1992\n",
      "2000\n",
      "2008\n",
      "2016\n",
      "2024\n",
      "2032\n",
      "2040\n",
      "2048\n",
      "2056\n",
      "2064\n",
      "2072\n",
      "2080\n",
      "2088\n",
      "2096\n",
      "2104\n",
      "2112\n",
      "2120\n",
      "2128\n",
      "2136\n",
      "2144\n",
      "2152\n",
      "2160\n",
      "2168\n",
      "2176\n",
      "2184\n",
      "2192\n",
      "2200\n",
      "2208\n",
      "2216\n",
      "2224\n",
      "2232\n",
      "2240\n",
      "2248\n",
      "2256\n",
      "2264\n",
      "2272\n",
      "2280\n",
      "2288\n",
      "2296\n",
      "2304\n",
      "2312\n",
      "2320\n",
      "2328\n",
      "2336\n",
      "2344\n",
      "2352\n",
      "2360\n",
      "2368\n",
      "2376\n",
      "2384\n",
      "2392\n",
      "2400\n",
      "2408\n",
      "2416\n",
      "2424\n",
      "2432\n",
      "2440\n",
      "2448\n",
      "2456\n",
      "2464\n",
      "2472\n",
      "2480\n",
      "2488\n",
      "2496\n",
      "2504\n",
      "2512\n",
      "2520\n",
      "2528\n",
      "2536\n",
      "2544\n",
      "2552\n",
      "2560\n",
      "2568\n",
      "2576\n",
      "2584\n",
      "2592\n",
      "2600\n",
      "2608\n",
      "2616\n",
      "2624\n",
      "2632\n",
      "2640\n",
      "2648\n",
      "2656\n",
      "2664\n",
      "2672\n",
      "2680\n",
      "2688\n",
      "2696\n",
      "2704\n",
      "2712\n",
      "2720\n",
      "2728\n",
      "2736\n",
      "2744\n",
      "2752\n",
      "2760\n",
      "2768\n",
      "2776\n",
      "2784\n",
      "2792\n",
      "2800\n",
      "2808\n",
      "2816\n",
      "2824\n",
      "2832\n",
      "2840\n",
      "2848\n",
      "2856\n",
      "2864\n",
      "2872\n",
      "2880\n",
      "2888\n",
      "2896\n",
      "2904\n",
      "2912\n",
      "2920\n",
      "2928\n",
      "2936\n",
      "2944\n",
      "2952\n",
      "2960\n",
      "2968\n",
      "2976\n",
      "2984\n",
      "2992\n",
      "3000\n",
      "3008\n",
      "3016\n",
      "3024\n",
      "3032\n",
      "3040\n",
      "3048\n",
      "3056\n",
      "3064\n",
      "3072\n",
      "3080\n",
      "3088\n",
      "3096\n",
      "3104\n",
      "3112\n",
      "3120\n",
      "3128\n",
      "3136\n",
      "3144\n",
      "3152\n",
      "3160\n",
      "3168\n",
      "3176\n",
      "3184\n",
      "3192\n",
      "3200\n",
      "3208\n",
      "3216\n",
      "3224\n",
      "3232\n",
      "3240\n",
      "3248\n",
      "3256\n",
      "3264\n",
      "3272\n",
      "3280\n",
      "3288\n",
      "3296\n",
      "3304\n",
      "3312\n",
      "3320\n",
      "3328\n",
      "3336\n",
      "3344\n",
      "3352\n",
      "3360\n",
      "3368\n",
      "3376\n",
      "3384\n",
      "3392\n",
      "3400\n",
      "3408\n",
      "3416\n",
      "3424\n",
      "3432\n",
      "3440\n",
      "3448\n",
      "3456\n",
      "3464\n",
      "3472\n",
      "3480\n",
      "3488\n",
      "3496\n",
      "3504\n",
      "3512\n",
      "3520\n",
      "3528\n",
      "3536\n",
      "3544\n",
      "3552\n",
      "3560\n",
      "3568\n",
      "3576\n",
      "3584\n",
      "3592\n",
      "3600\n",
      "3608\n",
      "3616\n",
      "3624\n",
      "3632\n",
      "3640\n",
      "3648\n",
      "3656\n",
      "3664\n",
      "3672\n",
      "3680\n",
      "3688\n",
      "3696\n",
      "3704\n",
      "3712\n",
      "3720\n",
      "3728\n",
      "3736\n",
      "3744\n",
      "3752\n",
      "3760\n",
      "3768\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=1)\n",
    "train_loader = DataLoader(train_data,batch_size=8, shuffle=True)\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer =torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    count=0\n",
    "    for X_batch,X_attention_batch,y_batch in train_loader:\n",
    "        print(count)\n",
    "        output =   model(X_batch,attention_mask=X_attention_batch,labels=None)\n",
    "        y_pred = output[0]\n",
    "        y_pred = torch.reshape(y_pred,(-1,))\n",
    "        loss = loss_fn(y_pred,y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        count+=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8857054377405722\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=False)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "preds = np.zeros([len(test_data), 1])\n",
    "model.eval()\n",
    "for i, (x_batch, x_mask) in enumerate(test_loader):\n",
    "    outputs = model(x_batch,attention_mask=x_mask)\n",
    "    y_pred = sigmoid(outputs[0].detach().numpy())\n",
    "    preds[i*8:(i+1)*8, :] = y_pred\n",
    "    \n",
    "print(metrics.roc_auc_score(test_targets, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/bert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
